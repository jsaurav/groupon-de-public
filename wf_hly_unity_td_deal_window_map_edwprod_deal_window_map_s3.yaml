# ****** Important - Do not remove any key from the config, even if you don't need them for your pipeline ******.
# All the fields which starts with m_ are mandatory and must have a value.
# All the fields which starts with o_ are optional and value may vary with different pipelines.
# All the keys which have value "DEFAULT", will read their values from SystemManager.
# All the keys which have value "DERIVED", will construct the value on the fly using global
  # variable and user provided input.
# ds1 is alias for target_table datasource and ds2 for lrf datasource. You will need this information later.
# Under m_dml, you have to provide implementation details of only the dml operation which you are invoking.
  # e.g. if you are using merge api, then you can remove other "delete","update","full_refresh","delete_insert","custom"
  # , "insert" from m_dml section.
# Please be careful with indentation, yaml expects tab indentation.


load_config:
  ############################################################################################################
  ###### USER INPUT SECTION  - This is the section where user has to provide the input specific to ###########
  ######                       there pipeline. All the fields starts with "m_" must have an user   ###########
  ######                       provided value.                                                     ###########
  ############################################################################################################
  # API Name which you want to invoke. Possible options are - merge, update, delete, delete_insert, full_refresh, custom
  # and insert.
  m_api_name: delete_insert
  # Name of the pipeline.
  m_pipeline_name: "wf_hly_unity_td_deal_window_map"
  # Service Name which you want to invoke.
  m_service_name: S3DeltaLakeService
  # DeltaLake Configuration starts here.
  m_delta:
    # Duration in hours to be passed to vacuum api. To delete all the older version of files
    # you should be passing some very small duration e.g. .000001
    # Default value is .000001
    o_time_travel_in_hrs: DEFAULT
    # Provide a list of columns on which your data is partitioned.
    # Please see below for an example. if your column name is "age" and
    # type is "integer", then value for this key would be "age integer"
    o_partition_column:
    # Name of the S3 bucket.
    m_bucket_name: DEFAULT
    # Bast Level. Possible values are high and low.
    # If data type is either c1 or c2,then please provide value as high.
    # If data type is c3 or c4 or c5 ,then please provide value as low.
    m_bast_level: low
    # Name of the target table.
    m_target_table_name: deal_window_map
    # Name of the target schema name.
    m_target_schema_name: edwprod
    # Location on S3 where target files (parquet) are present.
    # Value for this will be constructed, using global variables and
    # user provided input.
    # Pattern - s3://<bucket_name>/target/<m_bast_level>/<m_target_schema_name>/<m_target_table_name>
    m_target: DERIVED
    # Name of the dml operation which you want to perform on your data.
    # It's value will be derived from m_api_name above.
    m_dml_operation: DERIVED
    # Specify whether vacuum needs to be run or not after delta
    # process is completed. Default is False.
    o_run_vacuum: DEFAULT
    # DML operation detail starts here.
    # Here, you will be providing all details about the DML operation. See below for more details.
    # You have to provide implementation details of only the m_dml_operation
    # which you had mentioned above.
    # Other api implementation detail you can remove.
    # Please be careful with indentation.
    m_dml:
      delete_insert:
        # Join condition for LRF and target table.
        m_delete_insert_join_condition: "{ds1}.deal_uuid = {ds2}.deal_uuid"
        # Location on S3, where load ready files are present. These load ready files
        # will be used to load data back into target table.
        # Pattern - s3://<bucket_name>/pipeline/<pipeline_name>/lrf/<table_name>
        m_lrf_files_path: DERIVED
        # This flag controls if you want to dedup your lrf file based
        # on the join columns or custom columns.
        o_is_dedup_needed_for_delete: True
        # Columns on which dedup needs to be done.
        # If no value has been provided it will extract join columns
        # from the m_delete_insert_join_condition key.
        # If you are providing value, it should follow the below format.
        # e.g. o_dedup_columns: 'col1','col2','col3'
        o_dedup_columns: 'deal_uuid'
    # data in hive external table. msck repair needs to be done only for partitioned tables.
    m_run_msck_repair:
      # Set to TRUE if msck repair needs to be done after spark job is completed else set to FALSE.
      # Should be set to TRUE only for the partitioned tables.
      # DFEAULT value is False.
      m_is_msck_repair_required: DEFAULT
      # Name of the hive database where external table is created.
      m_database_name: None
      # Name of the external table.
      m_table_name: None
    # Details about the extra columns which can be added to the data frame.
    # These columns are not supposed to be business columns instead it should be
    # used for non-business columns. e.g. insert_user_id, insert_time.
    # You can provide 1 to many columns in the format below.
    # If you don't have any extra column to add, just don't add any of column keys
    # which starts with o_column.
    # Please use below format to provide extra columns.
    # Details of first column.
      #      o_column_1:
      #        # Name of the first column.
      #        m_column_name:
      #        # DataType of the first column.
      #        m_column_type:
      #        # Value of the first column.
      #        m_column_value:
    o_extra_columns:

  ############################################################################################################
  ##### DEFAULT SECTION -  In an ideal scenario you shouldn't be overriding any of the below values ##########
  #####                    unless you are 100% confident of what you are doing. All the values below #########
  #####                    will either be read from SystemsManager or will be derived using user input #######
  #####                    and global variables.                                                   ###########
  ############################################################################################################


  # Whether to stop the spark session on job completion or not.
  # This value should always be True, unless you are running unit tests.
  # Default value is True.
  m_stop_spark_session_on_completion: DEFAULT
  # Provide Cluster information.
  m_cluster:
    # Spark Application Name.
    # Default value will be pipeline name.
    m_app_name: DERIVED
    # Resource Manager for your cluster. Should always be yarn.
    # Default Value is yarn.
    m_master: DEFAULT
  # All the bookkeeping tasks will be defined here.
  # As of now only 3 types of tasks are defined.
  #   1. DeltaLogRenameTask
  #   2. StatusFileCreationTask
  #   3. AuditTableTask
  # If more type of tasks are added in future we will update you.
  m_book_keeping:
    # Name of the DeltaLogRenameTask
    m_t_delta_log_rename:
      # Should be set to TRUE if you want to rename _delta_log directory after spark job is completed else set False.
      m_is_delta_log_rename_required: DEFAULT
      # Name of the S3 bucket where this delta log directory will be moved after rename.
      m_bucket_name: DEFAULT
      # Location where _delta_log directory is created after the job completion.
      # In most of the cases this directory is created where your target table is present.
      # Pattern will be - s3://<bucket_name>/<target>/<db_name>/<table_name>/_delta_log.
      # DEFAULT value will be constructed using above pattern.
      m_old_delta_log_dir_name: DERIVED
      # Location where _delta_log directory will be copied.
      # Ideally it should be under backup/delta_logs directory under your pipeline on S3.
      # Pattern should be s3://<bucket_name>/<pipeline_name>/<table_name>/<backup>/delta_logs
      # DEFAULT value will be constructed using above pattern.
      m_new_delta_log_dir_path: DERIVED
      # This is an optional field, if you don't provide a name here, Pinion will generate
      # a new name using the following convention.
      # <pipeline_name>_<table_name>_delta_log_<time_stamp>
      o_new_delta_log_dir_name: DEFAULT
    # Name of the StatusFileCreationTask
    m_t_status_file_creation:
      # Should be set to TRUE if you want Pinion to generate a status file after spark job is completed else set False.
      m_is_status_file_required: DEFAULT
      # Name of the S3 bucket where status file will be created after spark job is completed.
      m_bucket_name: DEFAULT
      # Location on S3 where you want this status file to be created.
      # By default status file will be created under target table name.
      # Pattern - s3://<bucket_name>/<target>/<db_name>/<table_name>/_delta_log
      o_file_path: DERIVED
      # Name of the status file.
      # Default value is .done.
      m_file_name: DEFAULT
    # Name of the AuditTableTask.
    # This task will make an entry in JOB_INSTANCES & TABLE_LIMITS table.
    m_t_audit_table_entry:
      # Should be set to TRUE if you want to make an entry into audit table after spark job is completed else set False.
      # Default value is True.
      m_is_audit_table_entry_required: DEFAULT
      # Secret name of the connection on Secret Manager.
      # It will be read from
      m_secret_name: DEFAULT
      # region in which secret was created.
      m_region_name: DEFAULT
